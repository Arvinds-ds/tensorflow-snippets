{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import lookup_ops\n",
    "import numpy as np\n",
    "import codecs\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _preprocess(line):\n",
    "    '''\n",
    "    performs the following preprocessing on a line of text.\n",
    "    - lower case the text\n",
    "    - remove punctuations\n",
    "    - try alternate decoding if utf-8 fails\n",
    "    - encode string as utf-8\n",
    "    Note this is a pyfunc as tensorflow doesn't seem to have string operations beyond splitting\n",
    "    \n",
    "    Returns: a utf-8 encoded processed string\n",
    "    '''\n",
    "    try:\n",
    "        line = line.decode('utf-8')\n",
    "        line = line.lower()\n",
    "        line = line.strip()\n",
    "        line = re.sub('['+string.punctuation+']', '', line)\n",
    "    except:\n",
    "        try:\n",
    "            line = line.decode('iso-8859-1')\n",
    "            line = line.lower()\n",
    "            line = re.sub('['+string.punctuation+']', '', line)\n",
    "        except:\n",
    "            return line.lower()\n",
    "    return line.encode('utf-8')\n",
    "\n",
    "def _add_vocab_file_generator(text_files):\n",
    "    '''\n",
    "    generates a vocabulary file processing block to the current graph. It uses TF 1.2 new Dataset API\n",
    "    to process a list of files and split it in to tokens required for vocabulary generation\n",
    "    \n",
    "    Arguments: 1-D tensor of file names\n",
    "    Returns: A iterator that returns a list of tokens per line.\n",
    "    '''\n",
    "    with tf.name_scope('vocab_gen') as scope:\n",
    "        # create a dataset from list of file names\n",
    "        dataset = tf.contrib.data.Dataset.from_tensor_slices(tf.constant(text_files))\n",
    "        # generate a list of lines from files\n",
    "        dataset = dataset.flat_map(lambda filename: tf.contrib.data.TextLineDataset(filename))\n",
    "        # preprocess each line to lower case, remove punctuation chars\n",
    "        dataset = dataset.map(lambda line: tf.py_func(_preprocess,[line], [tf.string]))\n",
    "        # basic tokenization - space\n",
    "        dataset = dataset.map(lambda line: tf.string_split([line]).values)\n",
    "        # make a one shot iterator\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        next_line = iterator.get_next()\n",
    "        return next_line\n",
    "\n",
    "\n",
    "def gen_vocab_file(file_names, vocabulary_file='./vocab.txt', max_vocab_size=None,markers = True, reset_graph=False):\n",
    "    '''\n",
    "    generates the vocabulary file specified by vocabulary file by iterating through the iterator returned by\n",
    "    _add_vocab_file_generator.\n",
    "    \n",
    "    Arguments:\n",
    "    file_names - a list of filenames to process\n",
    "    vocabulary_file - the name of output file\n",
    "    max_vocab_size - the maximum number of words in the vocabulary. None implies all words will be included, else\n",
    "                     only the max_vocab_size common words will be included\n",
    "    markers - add the mandatory <UNK>, <SOS>, <EOS> markers to the beginning of vocab file\n",
    "    reset_graph - reset current graph before running this function.\n",
    "    \n",
    "    Note: Gives some issues on Windows with special characters in file names. Otherwise tested to work well on corpus with\n",
    "    a million files and up to 4 billion tokens\n",
    "    '''\n",
    "    if reset_graph is True:\n",
    "        tf.reset_default_graph()\n",
    "    \n",
    "    next_line = _add_vocab_file_generator(file_names)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        vocab = Counter()\n",
    "        logging.info('Start generating %s from %d files' % (vocabulary_file, len(file_names)))\n",
    "        start_time = time.time()\n",
    "        line_count = 0\n",
    "        word_count = 0\n",
    "        while True:\n",
    "            try:\n",
    "                word_list=next_line.eval()\n",
    "                vocab.update(word_list)\n",
    "                line_count +=1\n",
    "                word_count += len(word_list)\n",
    "                if line_count % 100000==0:\n",
    "                    logging.debug(\"%d lines and %d words processed\" % (line_count, word_count))\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                logging.debug(\"Completed:%d lines and %d words processed\" % (line_count, word_count))\n",
    "                break\n",
    "        vocab = vocab.most_common(max_vocab_size)\n",
    "        with  codecs.getreader(\"utf-8\")(tf.gfile.GFile(vocabulary_file, \"w\")) as vocab_file:\n",
    "            if markers is True:\n",
    "                vocab_file.write('<PAD>\\n<UNK>\\n<SOS>\\n<EOS>\\n')\n",
    "            for item in vocab:\n",
    "                vocab_file.write(\"{}\\n\".format(item[0].decode('utf-8')))\n",
    "        logging.info('Completed generating %s in %d s' % (vocabulary_file, time.time()-start_time))\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate vocabulary file from wiki data\n",
    "gen_vocab_file(['./textwiki8'],reset_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padded_sequences(batch_size,text_files,vocab_file='./vocab.txt',\n",
    "                            reset_graph=False):\n",
    "    '''\n",
    "    generate a padded sequence of data from source text files. Each line of the file\n",
    "    will be converted to a padded sequence of integers coded by vocabulary and length\n",
    "    of the sentence i.e\n",
    "    'Hello World there','Bye there' -> ([12, 10, 15],3),([21, 15, 0],2)\n",
    "    Arguments:\n",
    "    ----------\n",
    "    batch_size - size of batch requested\n",
    "    text_files - list of file names of text files containing data\n",
    "    vocab_file - path to vocabulary file containing the vocabulary for translating words\n",
    "                 to ids. This file must have the format\n",
    "                 <PAD>\n",
    "                 <UNK>\n",
    "                 <SOS>\n",
    "                 <EOS>\n",
    "                 word1\n",
    "                 word2\n",
    "                 ....\n",
    "    reset_graph - [optional], reset the graph\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    iterator - to generate a batch of data (of size batch size) of \n",
    "               (sentence ids, sentence length tuples) i.e\n",
    "               ([[12,10,15],[21,15,0]], [3,2])\n",
    "    vocab_table - tensorflow hashtable containing word to index mapping\n",
    "    reverse_vocab_table - tensorflow reverse hashtable containing index to word mapping\n",
    "    \n",
    "    '''\n",
    "    if reset_graph is True:\n",
    "        tf.reset_default_graph()\n",
    "    table = lookup_ops.index_table_from_file(vocab_file, num_oov_buckets=0,\n",
    "                                             default_value=1)\n",
    "    reverse_table = lookup_ops.index_to_string_table_from_file(vocab_file)\n",
    "    dataset = tf.contrib.data.Dataset.from_tensor_slices(tf.constant(text_files))\n",
    "    dataset = dataset.shuffle(buffer_size=100)\n",
    "    dataset = dataset.flat_map(lambda filename: tf.contrib.data.TextLineDataset(filename))\n",
    "    dataset = dataset = dataset.map(lambda line: tf.py_func(_preprocess,[line],\n",
    "                                                            [tf.string]))\n",
    "    dataset = dataset.map(lambda line: tf.string_split([line]).values)\n",
    "    dataset = dataset.map(lambda words: (table.lookup(words), tf.size(words)))\n",
    "    pad_value = tf.cast(table.lookup(tf.constant('<PAD>')),dtype=tf.int64)\n",
    "    dataset = dataset.padded_batch(batch_size,padded_shapes=(tf.TensorShape([None]),\n",
    "                                                             tf.TensorShape([])),\n",
    "                                   padding_values=(pad_value,0))\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    return iterator, table, reverse_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate padded sequence for one file with batch size of 5\n",
    "iterator, table, reverse_table = create_padded_sequences(5,['./test-text.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.tables_initializer())\n",
    "sess.run(iterator.initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4929   3087     15      9    185      5   2843     51     61    160\n",
      "     130    781    461  10180    137      4  25285      5      4    116\n",
      "     893      6      4  16153  53813      5      4    165    893]\n",
      " [  1673     87    123      8      7     17     26     24      4    187\n",
      "   69474     18    745      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0]\n",
      " [   242     88      4     51    187     92     50   3248    236   2140\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0]\n",
      " [  6491    137   4929    232     10     48   6659    164     10   1013\n",
      "    1865      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0]\n",
      " [  1893      4    187    145 113639     31      4      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0]]\n",
      "[29 13 10 11  7]\n"
     ]
    }
   ],
   "source": [
    "sent, sent_len = sess.run(iterator.get_next())\n",
    "print(sent)\n",
    "print(sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[b'anarchism', b'originated', b'as', b'a', b'term', b'of', b'abuse',\n",
       "        b'first', b'used', b'against', b'early', b'working', b'class',\n",
       "        b'radicals', b'including', b'the', b'diggers', b'of', b'the',\n",
       "        b'english', b'revolution', b'and', b'the', b'sans', b'culottes',\n",
       "        b'of', b'the', b'french', b'revolution'],\n",
       "       [b'twenty', b'years', b'later', b'in', b'one', b'eight', b'six',\n",
       "        b'four', b'the', b'international', b'workingmen', b's',\n",
       "        b'association', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>',\n",
       "        b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>',\n",
       "        b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>'],\n",
       "       [b'sometimes', b'called', b'the', b'first', b'international',\n",
       "        b'united', b'some', b'diverse', b'european', b'revolutionary',\n",
       "        b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>',\n",
       "        b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>',\n",
       "        b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>',\n",
       "        b'<PAD>'],\n",
       "       [b'currents', b'including', b'anarchism', b'due', b'to', b'its',\n",
       "        b'genuine', b'links', b'to', b'active', b'workers', b'<PAD>',\n",
       "        b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>',\n",
       "        b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>',\n",
       "        b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>'],\n",
       "       [b'movements', b'the', b'international', b'became', b'signficiant',\n",
       "        b'from', b'the', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>',\n",
       "        b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>',\n",
       "        b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>',\n",
       "        b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>', b'<PAD>']], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify reverse lookup works\n",
    "reverse_table.lookup(tf.constant(sent, dtype=tf.int64)).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
