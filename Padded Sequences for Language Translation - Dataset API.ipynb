{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import lookup_ops\n",
    "import numpy as np\n",
    "import codecs\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import unicodedata\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected file format (Source and Traget translation in one sentence seperated by TAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I forgot.\tमैं भूल गई।\n",
    "\n",
    "I'll pay.\tमैं पैसे दूंगा।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove punctuation characters in unicode strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tbl = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                      if unicodedata.category(chr(i)).startswith('P'))\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate vocabulary from source sentences in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _source_preprocess(line):\n",
    "    '''\n",
    "    performs the following preprocessing on a line of text.\n",
    "    - lower case the text\n",
    "    - remove punctuations\n",
    "    - try alternate decoding if utf-8 fails\n",
    "    - encode string as utf-8\n",
    "    Note this is a pyfunc as tensorflow doesn't seem to have string operations beyond splitting\n",
    "    \n",
    "    Returns: a utf-8 encoded processed string\n",
    "    '''\n",
    "    try:\n",
    "        line = line.decode('utf-8')\n",
    "    except:\n",
    "        try:\n",
    "            line = line.decode('iso-8859-1')\n",
    "        except:\n",
    "            return line.lower()\n",
    "    line = line.lower()\n",
    "    line = line.strip()\n",
    "    line = remove_punctuation(line)\n",
    "    return line.encode('utf-8')\n",
    "\n",
    "def _add_source_vocab_file_generator(text_files):\n",
    "    '''\n",
    "    generates a vocabulary file processing block to the current graph. It uses TF 1.2 new Dataset API\n",
    "    to process a list of files and split it in to tokens required for vocabulary generation\n",
    "    \n",
    "    Arguments: 1-D tensor of file names\n",
    "    Returns: A iterator that returns a list of tokens per line.\n",
    "    '''\n",
    "    with tf.name_scope('source_vocab_gen') as scope:\n",
    "        # create a dataset from list of file names\n",
    "        dataset = tf.contrib.data.Dataset.from_tensor_slices(tf.constant(text_files))\n",
    "        # generate a list of lines from files\n",
    "        dataset = dataset.flat_map(lambda filename: tf.contrib.data.TextLineDataset(filename))\n",
    "        # split only one part of language\n",
    "        dataset = dataset.map(lambda line: tf.string_split([line], delimiter='\\t').values[0])\n",
    "        # preprocess each line to lower case, remove punctuation chars\n",
    "        dataset = dataset.map(lambda line: tf.py_func(_source_preprocess,[line], [tf.string]))\n",
    "        # basic tokenization - space\n",
    "        dataset = dataset.map(lambda line: tf.string_split([line]).values)\n",
    "        # make a one shot iterator\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        next_line = iterator.get_next()\n",
    "        return next_line\n",
    "\n",
    "\n",
    "def gen_source_vocab_file(file_names, vocabulary_file='./data/source-vocab.txt', max_vocab_size=None,markers = True,\n",
    "                          reset_graph=False):\n",
    "    '''\n",
    "    generates the vocabulary file specified by vocabulary file by iterating through the iterator returned by\n",
    "    _add_vocab_file_generator.\n",
    "    \n",
    "    Arguments:\n",
    "    file_names - a list of filenames to process\n",
    "    source_vocabulary_file - the name of output vocabulary file for source\n",
    "    max_vocab_size - the maximum number of words in the vocabulary. None implies all words will be included, else\n",
    "                     only the max_vocab_size common words will be included\n",
    "    markers - add the mandatory <UNK>, <SOS>, <EOS> markers to the beginning of vocab file\n",
    "    reset_graph - reset current graph before running this function.\n",
    "    \n",
    "    Note: Gives some issues on Windows with special characters in file names. Otherwise tested to work well on corpus with\n",
    "    a million files and up to 4 billion tokens\n",
    "    '''\n",
    "    if reset_graph is True:\n",
    "        tf.reset_default_graph()\n",
    "    \n",
    "    next_line = _add_source_vocab_file_generator(file_names)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        vocab = Counter()\n",
    "        logging.info('Start generating %s from %d files' % (vocabulary_file, len(file_names)))\n",
    "        start_time = time.time()\n",
    "        line_count = 0\n",
    "        word_count = 0\n",
    "        while True:\n",
    "            try:\n",
    "                word_list=next_line.eval()\n",
    "                vocab.update(word_list)\n",
    "                line_count +=1\n",
    "                word_count += len(word_list)\n",
    "                if line_count % 100000==0:\n",
    "                    logging.debug(\"%d lines and %d words processed\" % (line_count, word_count))\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                logging.debug(\"Completed:%d lines and %d words processed\" % (line_count, word_count))\n",
    "                break\n",
    "        vocab = vocab.most_common(max_vocab_size)\n",
    "        with  codecs.getreader(\"utf-8\")(tf.gfile.GFile(vocabulary_file, \"w\")) as vocab_file:\n",
    "            if markers is True:\n",
    "                vocab_file.write('<PAD>\\n<UNK>\\n<SOS>\\n<EOS>\\n')\n",
    "            for item in vocab:\n",
    "                vocab_file.write(\"{}\\n\".format(item[0].decode('utf-8')))\n",
    "        logging.info('Completed generating %s in %d s' % (vocabulary_file, time.time()-start_time))\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate vocabulary from target sentences in files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _target_preprocess(line):\n",
    "    '''\n",
    "    performs the following preprocessing on a line of text.\n",
    "    - lower case the text\n",
    "    - remove punctuations\n",
    "    - try alternate decoding if utf-8 fails\n",
    "    - encode string as utf-8\n",
    "    Note this is a pyfunc as tensorflow doesn't seem to have string operations beyond splitting\n",
    "    \n",
    "    Returns: a utf-8 encoded processed string\n",
    "    '''\n",
    "    try:\n",
    "        line = line.decode('utf-8')\n",
    "    except:\n",
    "        try:\n",
    "            line = line.decode('iso-8859-1')\n",
    "        except:\n",
    "            return line.lower()\n",
    "    line = line.strip()\n",
    "    line = remove_punctuation(line)\n",
    "    return line.encode('utf-8')\n",
    "\n",
    "def _add_target_vocab_file_generator(text_files):\n",
    "    '''\n",
    "    generates a vocabulary file processing block to the current graph. It uses TF 1.2 new Dataset API\n",
    "    to process a list of files and split it in to tokens required for vocabulary generation\n",
    "    \n",
    "    Arguments: 1-D tensor of file names\n",
    "    Returns: A iterator that returns a list of tokens per line.\n",
    "    '''\n",
    "    with tf.name_scope('vocab_gen') as scope:\n",
    "        # create a dataset from list of file names\n",
    "        dataset = tf.contrib.data.Dataset.from_tensor_slices(tf.constant(text_files))\n",
    "        # generate a list of lines from files\n",
    "        dataset = dataset.flat_map(lambda filename: tf.contrib.data.TextLineDataset(filename))\n",
    "        # split only one part of language\n",
    "        dataset = dataset.map(lambda line: tf.string_split([line], delimiter='\\t').values[1])\n",
    "        # preprocess each line to lower case, remove punctuation chars\n",
    "        dataset = dataset.map(lambda line: tf.py_func(_target_preprocess,[line], [tf.string]))\n",
    "        # basic tokenization - space\n",
    "        dataset = dataset.map(lambda line: tf.string_split([line]).values)\n",
    "        # make a one shot iterator\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        next_line = iterator.get_next()\n",
    "        return next_line\n",
    "\n",
    "\n",
    "def gen_target_vocab_file(file_names, vocabulary_file='./data/vocab.txt', max_vocab_size=None,markers = True, reset_graph=False):\n",
    "    '''\n",
    "    generates the vocabulary file specified by vocabulary file by iterating through the iterator returned by\n",
    "    _add_vocab_file_generator.\n",
    "    \n",
    "    Arguments:\n",
    "    file_names - a list of filenames to process\n",
    "    vocabulary_file - the name of output file\n",
    "    max_vocab_size - the maximum number of words in the vocabulary. None implies all words will be included, else\n",
    "                     only the max_vocab_size common words will be included\n",
    "    markers - add the mandatory <UNK>, <SOS>, <EOS> markers to the beginning of vocab file\n",
    "    reset_graph - reset current graph before running this function.\n",
    "    \n",
    "    Note: Gives some issues on Windows with special characters in file names. Otherwise tested to work well on corpus with\n",
    "    a million files and up to 4 billion tokens\n",
    "    '''\n",
    "    if reset_graph is True:\n",
    "        tf.reset_default_graph()\n",
    "    \n",
    "    next_line = _add_target_vocab_file_generator(file_names)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        vocab = Counter()\n",
    "        logging.info('Start generating %s from %d files' % (vocabulary_file, len(file_names)))\n",
    "        start_time = time.time()\n",
    "        line_count = 0\n",
    "        word_count = 0\n",
    "        while True:\n",
    "            try:\n",
    "                word_list=next_line.eval()\n",
    "                vocab.update(word_list)\n",
    "                line_count +=1\n",
    "                word_count += len(word_list)\n",
    "                if line_count % 100000==0:\n",
    "                    logging.debug(\"%d lines and %d words processed\" % (line_count, word_count))\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                logging.debug(\"Completed:%d lines and %d words processed\" % (line_count, word_count))\n",
    "                break\n",
    "        vocab = vocab.most_common(max_vocab_size)\n",
    "        with  codecs.getreader(\"utf-8\")(tf.gfile.GFile(vocabulary_file, \"w\")) as vocab_file:\n",
    "            if markers is True:\n",
    "                vocab_file.write('<PAD>\\n<UNK>\\n<SOS>\\n<EOS>\\n')\n",
    "            for item in vocab:\n",
    "                vocab_file.write(\"{}\\n\".format(item[0].decode('utf-8')))\n",
    "        logging.info('Completed generating %s in %d s' % (vocabulary_file, time.time()-start_time))\n",
    "        sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Vocabulary files in source and target languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_source_vocab_file(['./data/en-hin.txt'],vocabulary_file='./data/en-vocab.txt',reset_graph=True)\n",
    "gen_target_vocab_file(['./data/en-hin.txt'],vocabulary_file='./data/hi-vocab.txt',reset_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create datasets in both languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_source_dataset(batch_size, text_files, vocab_file, reset_graph=False):\n",
    "    '''\n",
    "    generate a padded sequence of data from source text files. Each line of the file\n",
    "    will be converted to a padded sequence of integers coded by vocabulary and length\n",
    "    of the sentence i.e\n",
    "    'Hello World there','Bye there' -> ([12, 10, 15],3),([21, 15, 0],2)\n",
    "    Arguments:\n",
    "    ----------\n",
    "    batch_size - size of batch requested\n",
    "    text_files - list of file names of text files containing data\n",
    "    vocab_file - path to source vocabulary file containing the vocabulary for translating words\n",
    "                 to ids. This file must have the format\n",
    "                 <PAD>\n",
    "                 <UNK>\n",
    "                 <SOS>\n",
    "                 <EOS>\n",
    "                 word1\n",
    "                 word2\n",
    "                 ....\n",
    "    reset_graph - [optional], reset the graph\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    iterator - to generate a batch of data (of size batch size) of \n",
    "               (sentence ids, sentence length tuples) i.e\n",
    "               ([[12,10,15],[21,15,0]], [3,2])\n",
    "    vocab_table - tensorflow hashtable containing word to index mapping\n",
    "    reverse_vocab_table - tensorflow reverse hashtable containing index to word mapping\n",
    "    \n",
    "    '''\n",
    "    if reset_graph is True:\n",
    "        tf.reset_default_graph()\n",
    "    table = lookup_ops.index_table_from_file(vocab_file, num_oov_buckets=0,\n",
    "                                             default_value=1)\n",
    "    reverse_table = lookup_ops.index_to_string_table_from_file(vocab_file)\n",
    "    dataset = tf.contrib.data.Dataset.from_tensor_slices(tf.constant(text_files))\n",
    "    dataset = dataset.shuffle(buffer_size=100)\n",
    "    dataset = dataset.flat_map(lambda filename: tf.contrib.data.TextLineDataset(filename))\n",
    "    dataset = dataset.map(lambda line: tf.string_split([line], delimiter='\\t').values[0])\n",
    "    dataset = dataset.map(lambda line: tf.py_func(_source_preprocess,[line], [tf.string]))\n",
    "    dataset = dataset.map(lambda line: tf.string_split([line]).values)\n",
    "    dataset = dataset.map(lambda words: (table.lookup(words), tf.size(words)))\n",
    "    pad_value = tf.cast(table.lookup(tf.constant('<PAD>')),dtype=tf.int64)\n",
    "    dataset = dataset.padded_batch(batch_size,padded_shapes=(tf.TensorShape([None]),\n",
    "                                                             tf.TensorShape([])),\n",
    "                                                               padding_values=(pad_value,0))\n",
    "    return dataset, table, reverse_table\n",
    "    \n",
    "    \n",
    "def create_target_dataset(batch_size, text_files, vocab_file, reset_graph=False):\n",
    "    '''\n",
    "    generate a padded sequence of data from target text files. Each line of the file\n",
    "    will be converted to a padded sequence of integers coded by target vocabulary and length\n",
    "    of the sentence. Note However for decoder purposes, there will be 2 dataset tuples returned\n",
    "    target_in <- inputs to the decoder with <SOS> marker prefixed to sentence\n",
    "    target_out <- outputs of decoder with <EOS> market suffixed to sentence\n",
    "    'बचाओ','मैं ठीक हूँ' -> target_in:  ([2,12, 10, 15],4),([2,21, 15, 0],3)\n",
    "                         target_out:  ([12, 10, 15,3],4),([21, 15,3, 0],3)\n",
    "    Arguments:\n",
    "    ----------\n",
    "    batch_size - size of batch requested\n",
    "    text_files - list of file names of text files containing data\n",
    "    vocab_file - path to target vocabulary file containing the vocabulary for translating words\n",
    "                 to ids. This file must have the format\n",
    "                 <PAD>\n",
    "                 <UNK>\n",
    "                 <SOS>\n",
    "                 <EOS>\n",
    "                 word1\n",
    "                 word2\n",
    "                 ....\n",
    "    reset_graph - [optional], reset the graph\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    iterator - to generate a batch of data (of size batch size) of \n",
    "               (in_sentence ids, in_sentence length tuples) and (out_sentence ids, out_sentence length tuples)\n",
    "               i.e\n",
    "               ([[2,12,10,15],[2,21,15,0]], [4,3]),([[12,10,15,3],[21,15,3,0]], [4,3])\n",
    "               \n",
    "    vocab_table - tensorflow hashtable containing word to index mapping\n",
    "    reverse_vocab_table - tensorflow reverse hashtable containing index to word mapping\n",
    "    \n",
    "    '''\n",
    "    if reset_graph is True:\n",
    "        tf.reset_default_graph()\n",
    "    table = lookup_ops.index_table_from_file(vocab_file, num_oov_buckets=0,\n",
    "                                             default_value=1)\n",
    "    reverse_table = lookup_ops.index_to_string_table_from_file(vocab_file)\n",
    "    dataset = tf.contrib.data.Dataset.from_tensor_slices(tf.constant(text_files))\n",
    "    dataset = dataset.shuffle(buffer_size=100)\n",
    "    dataset = dataset.flat_map(lambda filename: tf.contrib.data.TextLineDataset(filename))\n",
    "    dataset = dataset.map(lambda line: tf.string_split([line], delimiter='\\t').values[1])\n",
    "    dataset = dataset.map(lambda line: tf.py_func(_source_preprocess,[line], [tf.string]))\n",
    "    dataset = dataset.map(lambda line: tf.string_split([line]).values)\n",
    "    dataset = dataset.map(lambda words: (table.lookup(words), tf.size(words)))\n",
    "    pad_value = tf.cast(table.lookup(tf.constant('<PAD>')),dtype=tf.int64)\n",
    "    start_value = tf.cast(table.lookup(tf.constant('<SOS>')),dtype=tf.int64)\n",
    "    end_value = tf.cast(table.lookup(tf.constant('<EOS>')),dtype=tf.int64)\n",
    "    dataset_in = dataset.map(lambda words, word_len: (tf.concat(([start_value],words),0), word_len+1))\n",
    "    dataset_out = dataset.map(lambda words, word_len: (tf.concat((words,[end_value]),0), word_len+1))\n",
    "    dataset_in = dataset_in.padded_batch(batch_size,padded_shapes=(tf.TensorShape([None]),\n",
    "                                                             tf.TensorShape([])),\n",
    "                                                               padding_values=(pad_value,0))\n",
    "    dataset_out = dataset_out.padded_batch(batch_size,padded_shapes=(tf.TensorShape([None]),\n",
    "                                                             tf.TensorShape([])),\n",
    "                                                               padding_values=(pad_value,0))\n",
    "    return dataset_in,dataset_out, table, reverse_table \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_dataset, source_table, source_reverse_table = create_source_dataset(5,['./data/en-hin.txt'],\n",
    "                                                                           vocab_file='./data/en-vocab.txt')\n",
    "target_dataset_in, target_dataset_out, target_table, target_reverse_table = \\\n",
    "                                                create_target_dataset(5,['./data/en-hin.txt'],\n",
    "                                                                      vocab_file='./data/hi-vocab.txt')\n",
    "source_iterator = source_dataset.make_initializable_iterator()\n",
    "target_in_iterator = target_dataset_in.make_initializable_iterator()\n",
    "target_out_iterator = target_dataset_out.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.tables_initializer())\n",
    "sess.run(source_iterator.initializer)\n",
    "sess.run(target_in_iterator.initializer)\n",
    "sess.run(target_out_iterator.initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 93   0]\n",
      " [898   0]\n",
      " [898   0]\n",
      " [132  14]\n",
      " [ 41 899]]\n",
      "[1 1 1 2 2]\n"
     ]
    }
   ],
   "source": [
    "sent, sent_len = sess.run(source_iterator.get_next())\n",
    "print(sent)\n",
    "print(sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['help', '<PAD>']\n",
      "['hello', '<PAD>']\n",
      "['hello', '<PAD>']\n",
      "['got', 'it']\n",
      "['im', 'ok']\n"
     ]
    }
   ],
   "source": [
    "source_str = [x for x in source_reverse_table.lookup(tf.constant(sent, dtype=tf.int64)).eval().tolist()]\n",
    "for i in range(len(source_str)):\n",
    "    print([x.decode('utf-8') for x in source_str[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2 1423    0    0]\n",
      " [   2 1424    0    0]\n",
      " [   2 1425    0    0]\n",
      " [   2  961   30    6]\n",
      " [   2    7  163   18]]\n",
      "[2 2 2 4 4]\n"
     ]
    }
   ],
   "source": [
    "sent, sent_len = sess.run(target_in_iterator.get_next())\n",
    "print(sent)\n",
    "print(sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'बचाओ', '<PAD>', '<PAD>']\n",
      "['<SOS>', 'नमस्ते', '<PAD>', '<PAD>']\n",
      "['<SOS>', 'नमस्कार', '<PAD>', '<PAD>']\n",
      "['<SOS>', 'समझे', 'कि', 'नहीं']\n",
      "['<SOS>', 'मैं', 'ठीक', 'हूँ']\n"
     ]
    }
   ],
   "source": [
    "#verify reverse lookup works\n",
    "target_str = [x for x in target_reverse_table.lookup(tf.constant(sent, dtype=tf.int64)).eval().tolist()]\n",
    "for i in range(len(target_str)):\n",
    "    print([x.decode('utf-8') for x in target_str[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1423    3    0    0]\n",
      " [1424    3    0    0]\n",
      " [1425    3    0    0]\n",
      " [ 961   30    6    3]\n",
      " [   7  163   18    3]]\n",
      "[2 2 2 4 4]\n"
     ]
    }
   ],
   "source": [
    "sent, sent_len = sess.run(target_out_iterator.get_next())\n",
    "print(sent)\n",
    "print(sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['बचाओ', '<EOS>', '<PAD>', '<PAD>']\n",
      "['नमस्ते', '<EOS>', '<PAD>', '<PAD>']\n",
      "['नमस्कार', '<EOS>', '<PAD>', '<PAD>']\n",
      "['समझे', 'कि', 'नहीं', '<EOS>']\n",
      "['मैं', 'ठीक', 'हूँ', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "target_str = [x for x in target_reverse_table.lookup(tf.constant(sent, dtype=tf.int64)).eval().tolist()]\n",
    "for i in range(len(target_str)):\n",
    "    print([x.decode('utf-8') for x in target_str[i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
